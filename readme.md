# GTDLBench

## Abstract

With its effective and wide real applications, deep learning attracts more and more attention nowadays. Many deep learning frameworks are introduced to facilitate its research, development and applications. However, few research has been conducted to measure the features of these frameworks. Thus, it still remains a critical problem that how to choose the optimal framework to design and implement specific deep learning models. This paper aims at introducing a benchmark to systematically measure the performance of these deep learning frameworks. Common datasets and models in deep learning are adopted in our experiments to show the advantages and disadvantages of these frameworks. And the performance metrics derived from both deep learning and systems field are introduced to present an easy and holistic approach.


## Metrics

* Training Time (s)
* Testing Time (s)
* Accuracy (%)

## Tutorials

* [Benchmarking on MNIST](tutorials/benchmarking_on_mnist.md)
* [Benchmarking on CIFAR-10](tutorials/benchmarking_on_cifar10.md) 

## Frameworks

[Frameworks](frameworks/frameworks.md)

* [Caffe](frameworks/caffe.md) Download
* [MXNet](frameworks/mxnet.md) Download
* [TensorFlow](frameworks/tensorflow.md) Download
* [Theano](frameworks/theano.md) Download 
* [Torch](frameworks/torch.md) Download


## Datasets

* [AT&T Faces](datasets/att_face_dataset.md) | [Download](https://drive.google.com/open?id=1ibW1KHYo_tE2ZIu0EfAmCR_p0Rros3M0)
* [Caltech-101](datasets/CALTECH101_datasets.md) | [Download]()
* [CIFAR-10](datasets/CIFAR-10_datasets.md) | [Download](https://drive.google.com/open?id=1n5oKcBgBI7_oEp9xroAIyJbHvVp0bZDe)
* [CIFAR-100](datasets/CIFAR-100_datasets.md) | [Download](https://drive.google.com/open?id=1N1u6jZacEgnT3t1aYEBd2y7Bnwb5FK-k)
* [MNIST](datasets/MNIST_datasets.md) | [Download](https://drive.google.com/file/d/11ZiNnV3YtpZ7d9afHZg0rtDRrmhha-1E/view?usp=sharing)

## Models

* [Default-on-MNIST](models/mnist)
* [Default-on-CIFAR-10](models/cifar10)